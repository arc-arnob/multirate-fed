{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "import dateutil\n",
    "\n",
    "# ------------------ MODEL COMPONENTS ------------------\n",
    "\n",
    "class TaskSpecificAttention(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(TaskSpecificAttention, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, input_dim)\n",
    "        self.residual_fc = nn.Linear(input_dim, input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attention_weights = F.softmax(self.fc(x), dim=-1)\n",
    "        context_vector = torch.tanh(x * attention_weights)\n",
    "        return x + self.residual_fc(context_vector)\n",
    "\n",
    "\n",
    "class SharedGlobalTemporalAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(SharedGlobalTemporalAttention, self).__init__()\n",
    "        self.fc = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.final_fc = nn.Linear(hidden_dim, 1)\n",
    "        self.residual_fc = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, x_list):\n",
    "        combined_hidden = torch.stack([x.mean(dim=1) for x in x_list], dim=1).mean(dim=1)\n",
    "        tanh_hidden = torch.tanh(self.fc(combined_hidden))\n",
    "        attention_scores = self.final_fc(tanh_hidden).squeeze(-1)\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1).unsqueeze(-1)\n",
    "        context_vector = combined_hidden * attention_weights\n",
    "        repeated_context = self.residual_fc(context_vector).unsqueeze(1)\n",
    "        return [torch.cat((x, repeated_context.repeat(1, x.size(1), 1), x * repeated_context), dim=-1) for x in x_list]\n",
    "\n",
    "\n",
    "class FATHOMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, window_size=32):\n",
    "        super(FATHOMModel, self).__init__()\n",
    "        self.task_attention = TaskSpecificAttention(input_dim)\n",
    "        self.lstm1 = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(hidden_dim * 3, hidden_dim, batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, shared_context):\n",
    "        x = self.task_attention(x)\n",
    "        x, _ = self.lstm1(x)\n",
    "        enriched_context = torch.cat((x, shared_context, x * shared_context), dim=-1)\n",
    "        x, _ = self.lstm2(enriched_context)\n",
    "        x = torch.cat((x[:, -1, :], shared_context[:, -1, :]), dim=-1)\n",
    "        return self.fc2(F.relu(self.fc1(x)))\n",
    "\n",
    "\n",
    "class MultiTaskFATHOM(nn.Module):\n",
    "    def __init__(self, num_tasks, input_dim, hidden_dim, output_dim, window_size=32):\n",
    "        super(MultiTaskFATHOM, self).__init__()\n",
    "        self.shared_global_attention = SharedGlobalTemporalAttention(hidden_dim)\n",
    "        self.tasks = nn.ModuleList([\n",
    "            FATHOMModel(input_dim, hidden_dim, output_dim, window_size) for _ in range(num_tasks)\n",
    "        ])\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        first_stage_outputs = []\n",
    "        for task_model, x in zip(self.tasks, inputs):\n",
    "            x, _ = task_model.lstm1(task_model.task_attention(x))\n",
    "            first_stage_outputs.append(x)\n",
    "\n",
    "        shared_contexts = self.shared_global_attention(first_stage_outputs)\n",
    "        return [task_model(x, shared_context) for task_model, x, shared_context in zip(self.tasks, inputs, shared_contexts)]\n",
    "\n",
    "\n",
    "# ------------------ DATA LOADER ------------------\n",
    "\n",
    "def df_to_X_y(df, features, target, window_size=32, horizon=1):\n",
    "    if target not in features:\n",
    "        features = [target] + features\n",
    "\n",
    "    data = df[features].to_numpy()\n",
    "    target_data = df[target].to_numpy()\n",
    "\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - window_size - horizon + 1):\n",
    "        X.append(data[i:i + window_size])\n",
    "        y.append(target_data[i + window_size: i + window_size + horizon])\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "def load_and_preprocess_site_data(site_path, features, target, window_size=32, horizon=1, min_date=None, max_date=None, batch_size=16, device='cpu'):\n",
    "    df = pd.read_csv(site_path)\n",
    "\n",
    "    if 'date' in df.columns:\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        if min_date:\n",
    "            min_date = dateutil.parser.parse(min_date) if isinstance(min_date, str) else min_date\n",
    "            df = df[df['date'] >= min_date]\n",
    "        if max_date:\n",
    "            max_date = dateutil.parser.parse(max_date) if isinstance(max_date, str) else max_date\n",
    "            df = df[df['date'] <= max_date]\n",
    "        df.drop(columns=['date'], inplace=True)\n",
    "\n",
    "    if target not in features:\n",
    "        features = [target] + features\n",
    "\n",
    "    all_columns = features\n",
    "    if not all(col in df.columns for col in all_columns):\n",
    "        missing = [col for col in all_columns if col not in df.columns]\n",
    "        raise ValueError(f\"Missing columns in dataset: {missing}\")\n",
    "\n",
    "    train_size = int(0.8 * len(df))\n",
    "    train_df = df.iloc[:train_size]\n",
    "    test_df = df.iloc[train_size:]\n",
    "\n",
    "    val_size = int(0.2 * len(train_df))\n",
    "    train_df, val_df = train_df.iloc[:-val_size], train_df.iloc[-val_size:]\n",
    "\n",
    "    print(f\"Train size: {len(train_df)} | Validation size: {len(val_df)} | Test size: {len(test_df)}\")\n",
    "\n",
    "    train_mean, train_std = train_df[all_columns].mean(), train_df[all_columns].std()\n",
    "    train_df[all_columns] = (train_df[all_columns] - train_mean) / (train_std + 1e-8)\n",
    "    val_df[all_columns] = (val_df[all_columns] - train_mean) / (train_std + 1e-8)\n",
    "    test_df[all_columns] = (test_df[all_columns] - train_mean) / (train_std + 1e-8)\n",
    "\n",
    "    X_train, y_train = df_to_X_y(train_df, features, target, window_size, horizon)\n",
    "    X_val, y_val = df_to_X_y(val_df, features, target, window_size, horizon)\n",
    "    X_test, y_test = df_to_X_y(test_df, features, target, window_size, horizon)\n",
    "\n",
    "    train_data = TensorDataset(torch.tensor(X_train, dtype=torch.float32).to(device), torch.tensor(y_train, dtype=torch.float32).to(device))\n",
    "    val_data = TensorDataset(torch.tensor(X_val, dtype=torch.float32).to(device), torch.tensor(y_val, dtype=torch.float32).to(device))\n",
    "    test_data = TensorDataset(torch.tensor(X_test, dtype=torch.float32).to(device), torch.tensor(y_test, dtype=torch.float32).to(device))\n",
    "\n",
    "    train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "    val_loader = DataLoader(val_data, shuffle=False, batch_size=batch_size, drop_last=True)\n",
    "    test_loader = DataLoader(test_data, shuffle=False, batch_size=batch_size, drop_last=True)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "# ------------------ TRAINING & EVALUATION ------------------\n",
    "\n",
    "def train_fathom_model(site_loaders, model, optimizer, criterion, scheduler, num_epochs, device):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_losses, val_losses = [], []\n",
    "        for task_id, (train_loader, val_loader, _) in enumerate(site_loaders):\n",
    "            # Training phase\n",
    "            for X, y in train_loader:\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                preds = model([X])[task_id]\n",
    "                loss = criterion(preds, y.view(y.size(0), -1))\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_losses.append(loss.item())\n",
    "\n",
    "            # Validation phase\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_task_losses = []\n",
    "                for X_val, y_val in val_loader:\n",
    "                    X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "                    preds_val = model([X_val])[task_id]\n",
    "                    val_loss = criterion(preds_val, y_val.view(y_val.size(0), -1))\n",
    "                    val_task_losses.append(val_loss.item())\n",
    "                val_losses.append(np.mean(val_task_losses))\n",
    "            model.train()\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs} | Train Loss: {np.mean(train_losses):.4f} | Validation Loss: {np.mean(val_losses):.4f}\")\n",
    "\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "\n",
    "def evaluate_fathom_model(model, site_loaders, device='cpu'):\n",
    "    model.eval()\n",
    "    mae_scores = []\n",
    "    with torch.no_grad():\n",
    "        for task_id, (_, _, test_loader) in enumerate(site_loaders):\n",
    "            preds, targets = [], []\n",
    "            for X, y in test_loader:\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                preds.append(model([X])[task_id].cpu().numpy())\n",
    "                targets.append(y.cpu().numpy())\n",
    "            mae_scores.append(mean_absolute_error(np.concatenate(targets), np.concatenate(preds)))\n",
    "    print(\"Evaluation complete.\")\n",
    "    return mae_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    num_tasks = 3\n",
    "    batch_size, window_size, input_dim, hidden_dim, output_dim = 32, 32, 15, 64, 16\n",
    "\n",
    "    model = MultiTaskFATHOM(num_tasks, input_dim, hidden_dim, output_dim, window_size).to(\"cpu\")\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.9)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Mock data (replace with DataLoader-based site_loaders)\n",
    "    site_paths = [\"site_1.csv\", \"site_2.csv\", \"site_3.csv\"]\n",
    "    features = [\"feature1\", \"feature2\", \"feature3\"]\n",
    "    target = \"target\"\n",
    "    site_loaders = [load_and_preprocess_site_data(site_path, features, target, window_size, horizon=1, batch_size=batch_size) for site_path in site_paths]\n",
    "\n",
    "    train_fathom_model(site_loaders, model, optimizer, criterion, scheduler, num_epochs=5, device=\"cpu\")\n",
    "    mae_scores = evaluate_fathom_model(model, site_loaders, device=\"cpu\")\n",
    "    print(f\"MAE per task: {mae_scores}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Multiple-time-series",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
