{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "import dateutil\n",
    "\n",
    "# ------------------ MODEL COMPONENTS ------------------\n",
    "\n",
    "class TaskSpecificAttention(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(TaskSpecificAttention, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, input_dim)\n",
    "        self.residual_fc = nn.Linear(input_dim, input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attention_weights = F.softmax(self.fc(x), dim=-1)\n",
    "        context_vector = torch.tanh(x * attention_weights)\n",
    "        return x + self.residual_fc(context_vector)\n",
    "\n",
    "\n",
    "class SharedGlobalTemporalAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(SharedGlobalTemporalAttention, self).__init__()\n",
    "        self.fc = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.final_fc = nn.Linear(hidden_dim, 1)\n",
    "        self.residual_fc = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, x_list):\n",
    "        combined_hidden = torch.stack([x.mean(dim=1) for x in x_list], dim=1).mean(dim=1)\n",
    "        tanh_hidden = torch.tanh(self.fc(combined_hidden))\n",
    "        attention_scores = self.final_fc(tanh_hidden).squeeze(-1)\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1).unsqueeze(-1)\n",
    "        context_vector = combined_hidden * attention_weights\n",
    "        repeated_context = self.residual_fc(context_vector).unsqueeze(1)\n",
    "        return [torch.cat((x, repeated_context.repeat(1, x.size(1), 1), x * repeated_context), dim=-1) for x in x_list]\n",
    "\n",
    "\n",
    "class FATHOMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, window_size=32):\n",
    "        super(FATHOMModel, self).__init__()\n",
    "        self.task_attention = TaskSpecificAttention(input_dim)\n",
    "        self.lstm1 = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(hidden_dim * 3, hidden_dim, batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, shared_context):\n",
    "        x = self.task_attention(x)\n",
    "        x, _ = self.lstm1(x)\n",
    "        enriched_context = torch.cat((x, shared_context, x * shared_context), dim=-1)\n",
    "        x, _ = self.lstm2(enriched_context)\n",
    "        x = torch.cat((x[:, -1, :], shared_context[:, -1, :]), dim=-1)\n",
    "        return self.fc2(F.relu(self.fc1(x)))\n",
    "\n",
    "\n",
    "class MultiTaskFATHOM(nn.Module):\n",
    "    def __init__(self, num_tasks, input_dim, hidden_dim, output_dim, window_size=32):\n",
    "        super(MultiTaskFATHOM, self).__init__()\n",
    "        self.shared_global_attention = SharedGlobalTemporalAttention(hidden_dim)\n",
    "        self.tasks = nn.ModuleList([\n",
    "            FATHOMModel(input_dim, hidden_dim, output_dim, window_size) for _ in range(num_tasks)\n",
    "        ])\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        first_stage_outputs = []\n",
    "        for task_model, x in zip(self.tasks, inputs):\n",
    "            x, _ = task_model.lstm1(task_model.task_attention(x))\n",
    "            first_stage_outputs.append(x)\n",
    "\n",
    "        shared_contexts = self.shared_global_attention(first_stage_outputs)\n",
    "        return [task_model(x, shared_context) for task_model, x, shared_context in zip(self.tasks, inputs, shared_contexts)]\n",
    "\n",
    "\n",
    "# ------------------ DATA LOADER ------------------\n",
    "\n",
    "def df_to_X_y(df, features, target, window_size=32, horizon=1):\n",
    "    if target not in features:\n",
    "        features = [target] + features\n",
    "\n",
    "    data = df[features].to_numpy()\n",
    "    target_data = df[target].to_numpy()\n",
    "\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - window_size - horizon + 1):\n",
    "        X.append(data[i:i + window_size])\n",
    "        y.append(target_data[i + window_size: i + window_size + horizon])\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "def load_and_preprocess_site_data(site_path, features, target, window_size=32, horizon=1, min_date=None, max_date=None, batch_size=16, device='cpu'):\n",
    "    df = pd.read_csv(site_path)\n",
    "\n",
    "    if 'date' in df.columns:\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        if min_date:\n",
    "            min_date = dateutil.parser.parse(min_date) if isinstance(min_date, str) else min_date\n",
    "            df = df[df['date'] >= min_date]\n",
    "        if max_date:\n",
    "            max_date = dateutil.parser.parse(max_date) if isinstance(max_date, str) else max_date\n",
    "            df = df[df['date'] <= max_date]\n",
    "        df.drop(columns=['date'], inplace=True)\n",
    "\n",
    "    if target not in features:\n",
    "        features = [target] + features\n",
    "\n",
    "    all_columns = features\n",
    "    if not all(col in df.columns for col in all_columns):\n",
    "        missing = [col for col in all_columns if col not in df.columns]\n",
    "        raise ValueError(f\"Missing columns in dataset: {missing}\")\n",
    "\n",
    "    train_size = int(0.8 * len(df))\n",
    "    train_df = df.iloc[:train_size]\n",
    "    test_df = df.iloc[train_size:]\n",
    "\n",
    "    val_size = int(0.2 * len(train_df))\n",
    "    train_df, val_df = train_df.iloc[:-val_size], train_df.iloc[-val_size:]\n",
    "\n",
    "    print(f\"Train size: {len(train_df)} | Validation size: {len(val_df)} | Test size: {len(test_df)}\")\n",
    "\n",
    "    train_mean, train_std = train_df[all_columns].mean(), train_df[all_columns].std()\n",
    "    train_df[all_columns] = (train_df[all_columns] - train_mean) / (train_std + 1e-8)\n",
    "    val_df[all_columns] = (val_df[all_columns] - train_mean) / (train_std + 1e-8)\n",
    "    test_df[all_columns] = (test_df[all_columns] - train_mean) / (train_std + 1e-8)\n",
    "\n",
    "    X_train, y_train = df_to_X_y(train_df, features, target, window_size, horizon)\n",
    "    X_val, y_val = df_to_X_y(val_df, features, target, window_size, horizon)\n",
    "    X_test, y_test = df_to_X_y(test_df, features, target, window_size, horizon)\n",
    "\n",
    "    train_data = TensorDataset(torch.tensor(X_train, dtype=torch.float32).to(device), torch.tensor(y_train, dtype=torch.float32).to(device))\n",
    "    val_data = TensorDataset(torch.tensor(X_val, dtype=torch.float32).to(device), torch.tensor(y_val, dtype=torch.float32).to(device))\n",
    "    test_data = TensorDataset(torch.tensor(X_test, dtype=torch.float32).to(device), torch.tensor(y_test, dtype=torch.float32).to(device))\n",
    "\n",
    "    train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "    val_loader = DataLoader(val_data, shuffle=False, batch_size=batch_size, drop_last=True)\n",
    "    test_loader = DataLoader(test_data, shuffle=False, batch_size=batch_size, drop_last=True)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "# ------------------ TRAINING & EVALUATION ------------------\n",
    "\n",
    "def train_fathom_model(site_loaders, model, optimizer, criterion, scheduler, num_epochs, device):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_losses, val_losses = [], []\n",
    "        for task_id, (train_loader, val_loader, _) in enumerate(site_loaders):\n",
    "            # Training phase\n",
    "            for X, y in train_loader:\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                preds = model([X])[task_id]\n",
    "                loss = criterion(preds, y.view(y.size(0), -1))\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_losses.append(loss.item())\n",
    "\n",
    "            # Validation phase\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_task_losses = []\n",
    "                for X_val, y_val in val_loader:\n",
    "                    X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "                    preds_val = model([X_val])[task_id]\n",
    "                    val_loss = criterion(preds_val, y_val.view(y_val.size(0), -1))\n",
    "                    val_task_losses.append(val_loss.item())\n",
    "                val_losses.append(np.mean(val_task_losses))\n",
    "            model.train()\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs} | Train Loss: {np.mean(train_losses):.4f} | Validation Loss: {np.mean(val_losses):.4f}\")\n",
    "\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "\n",
    "def evaluate_fathom_model(model, site_loaders, device='cpu'):\n",
    "    model.eval()\n",
    "    mae_scores = []\n",
    "    with torch.no_grad():\n",
    "        for task_id, (_, _, test_loader) in enumerate(site_loaders):\n",
    "            preds, targets = [], []\n",
    "            for X, y in test_loader:\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                preds.append(model([X])[task_id].cpu().numpy())\n",
    "                targets.append(y.cpu().numpy())\n",
    "            mae_scores.append(mean_absolute_error(np.concatenate(targets), np.concatenate(preds)))\n",
    "    print(\"Evaluation complete.\")\n",
    "    return mae_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "import dateutil\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# ------------------ MODEL COMPONENTS ------------------\n",
    "\n",
    "class TaskSpecificAttention(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(TaskSpecificAttention, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, input_dim)\n",
    "        self.residual_fc = nn.Linear(input_dim, input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attention_weights = F.softmax(self.fc(x), dim=-1)\n",
    "        context_vector = torch.tanh(x * attention_weights)\n",
    "        return x + self.residual_fc(context_vector)\n",
    "\n",
    "\n",
    "class SharedGlobalTemporalAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(SharedGlobalTemporalAttention, self).__init__()\n",
    "        self.fc = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.final_fc = nn.Linear(hidden_dim, 1)\n",
    "        self.residual_fc = nn.Linear(hidden_dim, hidden_dim)\n",
    "        # Projection layer to match hidden_dim after concatenation\n",
    "        self.projection_layer = nn.Linear(hidden_dim * 3, hidden_dim)\n",
    "\n",
    "    def forward(self, x_list):\n",
    "        combined_hidden = torch.stack([x.mean(dim=1) for x in x_list], dim=1).mean(dim=1)\n",
    "        tanh_hidden = torch.tanh(self.fc(combined_hidden))\n",
    "        attention_scores = self.final_fc(tanh_hidden).squeeze(-1)\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1).unsqueeze(-1)\n",
    "        context_vector = combined_hidden * attention_weights\n",
    "        repeated_context = self.residual_fc(context_vector).unsqueeze(1)\n",
    "        # Concatenation and projection to hidden_dim\n",
    "        enriched_context_list = [\n",
    "            self.projection_layer(torch.cat((x, repeated_context.repeat(1, x.size(1), 1), x * repeated_context), dim=-1))\n",
    "            for x in x_list\n",
    "        ]\n",
    "        return enriched_context_list\n",
    "\n",
    "\n",
    "class FATHOMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, window_size=32):\n",
    "        super(FATHOMModel, self).__init__()\n",
    "        self.task_attention = TaskSpecificAttention(input_dim)\n",
    "        self.lstm1 = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(hidden_dim * 3, hidden_dim, batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, shared_context):\n",
    "        x = self.task_attention(x)\n",
    "        x, _ = self.lstm1(x)\n",
    "        enriched_context = torch.cat((x, shared_context, x * shared_context), dim=-1)\n",
    "        x, _ = self.lstm2(enriched_context)\n",
    "        x = torch.cat((x[:, -1, :], shared_context[:, -1, :]), dim=-1)\n",
    "        return self.fc2(F.relu(self.fc1(x)))\n",
    "\n",
    "\n",
    "class MultiTaskFATHOM(nn.Module):\n",
    "    def __init__(self, num_tasks, input_dim, hidden_dim, output_dim, window_size=32):\n",
    "        super(MultiTaskFATHOM, self).__init__()\n",
    "        self.shared_global_attention = SharedGlobalTemporalAttention(hidden_dim)\n",
    "        self.tasks = nn.ModuleList([\n",
    "            FATHOMModel(input_dim, hidden_dim, output_dim, window_size) for _ in range(num_tasks)\n",
    "        ])\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # n sites, n tensors: [batch_size, window_size, input_dim]\n",
    "        # print(len(inputs))\n",
    "        assert len(inputs) == len(self.tasks), (\n",
    "            f\"Mismatch: Received {len(inputs)} inputs but expected {len(self.tasks)} tasks.\"\n",
    "        )\n",
    "        first_stage_outputs = []\n",
    "        for task_model, x in zip(self.tasks, inputs):\n",
    "            x, _ = task_model.lstm1(task_model.task_attention(x))\n",
    "            first_stage_outputs.append(x)\n",
    "\n",
    "        shared_contexts = self.shared_global_attention(first_stage_outputs)\n",
    "        outputs = []\n",
    "        for i, (task_model, x, shared_context) in enumerate(zip(self.tasks, inputs, shared_contexts)):\n",
    "            preds = task_model(x, shared_context)\n",
    "            outputs.append(preds)\n",
    "            # print(f\"Task {i + 1}: Output shape {preds.shape}\")\n",
    "        return outputs\n",
    "\n",
    "\n",
    "# ------------------ DATA LOADER ------------------\n",
    "\n",
    "def df_to_X_y(df, features, target, window_size=32, horizon=1):\n",
    "    if target not in features:\n",
    "        features = [target] + features\n",
    "\n",
    "    data = df[features].to_numpy()\n",
    "    target_data = df[target].to_numpy()\n",
    "\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - window_size - horizon + 1):\n",
    "        X.append(data[i:i + window_size])\n",
    "        y.append(target_data[i + window_size: i + window_size + horizon])\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "def load_and_preprocess_site_data(site_path, features, target, window_size=32, horizon=1, min_date=None, max_date=None, batch_size=16, device='cpu'):\n",
    "    df = pd.read_csv(site_path)\n",
    "\n",
    "    if 'date' in df.columns:\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        if min_date:\n",
    "            min_date = dateutil.parser.parse(min_date) if isinstance(min_date, str) else min_date\n",
    "            df = df[df['date'] >= min_date]\n",
    "        if max_date:\n",
    "            max_date = dateutil.parser.parse(max_date) if isinstance(max_date, str) else max_date\n",
    "            df = df[df['date'] <= max_date]\n",
    "        df.drop(columns=['date'], inplace=True)\n",
    "\n",
    "    if target not in features:\n",
    "        features = [target] + features\n",
    "\n",
    "    all_columns = features\n",
    "    if not all(col in df.columns for col in all_columns):\n",
    "        missing = [col for col in all_columns if col not in df.columns]\n",
    "        raise ValueError(f\"Missing columns in dataset: {missing}\")\n",
    "\n",
    "    train_size = int(0.8 * len(df))\n",
    "    train_df = df.iloc[:train_size]\n",
    "    test_df = df.iloc[train_size:]\n",
    "\n",
    "    val_size = int(0.2 * len(train_df))\n",
    "    train_df, val_df = train_df.iloc[:-val_size], train_df.iloc[-val_size:]\n",
    "\n",
    "    print(f\"Train size: {len(train_df)} | Validation size: {len(val_df)} | Test size: {len(test_df)}\")\n",
    "\n",
    "    train_mean, train_std = train_df[all_columns].mean(), train_df[all_columns].std()\n",
    "    train_df[all_columns] = (train_df[all_columns] - train_mean) / (train_std + 1e-8)\n",
    "    val_df[all_columns] = (val_df[all_columns] - train_mean) / (train_std + 1e-8)\n",
    "    test_df[all_columns] = (test_df[all_columns] - train_mean) / (train_std + 1e-8)\n",
    "    \n",
    "    # scaler = MinMaxScaler()\n",
    "    # train_df[all_columns] = scaler.fit_transform(train_df[all_columns])\n",
    "    # val_df[all_columns] = scaler.transform(val_df[all_columns])\n",
    "    # test_df[all_columns] = scaler.transform(test_df[all_columns])\n",
    "\n",
    "    X_train, y_train = df_to_X_y(train_df, features, target, window_size, horizon)\n",
    "    X_val, y_val = df_to_X_y(val_df, features, target, window_size, horizon)\n",
    "    X_test, y_test = df_to_X_y(test_df, features, target, window_size, horizon)\n",
    "\n",
    "    train_data = TensorDataset(torch.tensor(X_train, dtype=torch.float32).to(device), torch.tensor(y_train, dtype=torch.float32).to(device))\n",
    "    val_data = TensorDataset(torch.tensor(X_val, dtype=torch.float32).to(device), torch.tensor(y_val, dtype=torch.float32).to(device))\n",
    "    test_data = TensorDataset(torch.tensor(X_test, dtype=torch.float32).to(device), torch.tensor(y_test, dtype=torch.float32).to(device))\n",
    "\n",
    "    train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "    val_loader = DataLoader(val_data, shuffle=False, batch_size=batch_size, drop_last=True)\n",
    "    test_loader = DataLoader(test_data, shuffle=False, batch_size=batch_size, drop_last=True)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "# ------------------ TRAINING & EVALUATION ------------------\n",
    "\n",
    "def train_fathom_model(site_loaders, model, optimizer, criterion, scheduler, num_epochs, device):\n",
    "    # Unpack loaders for each task\n",
    "    train_loaders = [loader_tuple[0] for loader_tuple in site_loaders]\n",
    "    val_loaders = [loader_tuple[1] for loader_tuple in site_loaders]\n",
    "    num_tasks = len(site_loaders)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        # Iterate over batches from all tasks simultaneously\n",
    "        for batches in zip(*train_loaders):\n",
    "            # Each batch in batches is a tuple (X, y) for a given task\n",
    "            Xs = [batch[0].to(device) for batch in batches]\n",
    "            ys = [batch[1].to(device) for batch in batches]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            # Pass the list of task batches to the model\n",
    "            preds_list = model(Xs)  # expects a list of tensors, one per task\n",
    "            \n",
    "            # Compute losses for each task and sum them\n",
    "            losses = [\n",
    "                criterion(pred, y.view(y.size(0), -1))\n",
    "                for pred, y in zip(preds_list, ys)\n",
    "            ]\n",
    "            total_loss = sum(losses)\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(total_loss.item())\n",
    "        \n",
    "        # Validation phase (similarly, iterate over all task validation loaders)\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for batches in zip(*val_loaders):\n",
    "                Xs = [batch[0].to(device) for batch in batches]\n",
    "                ys = [batch[1].to(device) for batch in batches]\n",
    "                preds_list = model(Xs)\n",
    "                losses = [\n",
    "                    criterion(pred, y.view(y.size(0), -1)).item()\n",
    "                    for pred, y in zip(preds_list, ys)\n",
    "                ]\n",
    "                # Average loss over tasks for this batch\n",
    "                val_losses.append(sum(losses) / num_tasks)\n",
    "        \n",
    "        scheduler.step()\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs} | Train Loss: {np.mean(train_losses):.4f} | Validation Loss: {np.mean(val_losses):.4f}\")\n",
    "    \n",
    "    print(\"Training complete.\")\n",
    "\n",
    "\n",
    "def evaluate_fathom_model(model, site_loaders, device='cpu'):\n",
    "    model.eval()\n",
    "    # Prepare test loaders from site_loaders\n",
    "    test_loaders = [loader_tuple[2] for loader_tuple in site_loaders]\n",
    "    task_preds, task_targets = [[] for _ in range(len(test_loaders))], [[] for _ in range(len(test_loaders))]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batches in zip(*test_loaders):\n",
    "            Xs = [batch[0].to(device) for batch in batches]\n",
    "            ys = [batch[1].to(device) for batch in batches]\n",
    "            preds_list = model(Xs)\n",
    "            for i, (pred, y) in enumerate(zip(preds_list, ys)):\n",
    "                task_preds[i].append(pred.cpu().numpy())\n",
    "                task_targets[i].append(y.cpu().numpy())\n",
    "    \n",
    "    # Compute MAE for each task\n",
    "    mae_scores = []\n",
    "    for preds, targets in zip(task_preds, task_targets):\n",
    "        preds_concat = np.concatenate(preds)\n",
    "        targets_concat = np.concatenate(targets)\n",
    "        mae_scores.append(mean_absolute_error(targets_concat, preds_concat))\n",
    "    \n",
    "    print(\"Evaluation complete.\")\n",
    "    return mae_scores\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_predictions_vs_ground_truth(model, site_loaders, device='cpu', window_size=32, stride=16):\n",
    "    \"\"\"\n",
    "    Plots the model predictions vs. ground truth for each task using the test DataLoader.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained multi-task model.\n",
    "        site_loaders: List of tuples (train_loader, val_loader, test_loader) for each site.\n",
    "        device: \"cpu\" or \"cuda\".\n",
    "        window_size: Sliding window size used during training.\n",
    "        stride: Plot every 'stride'-th point to reduce clutter.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    # Extract test loaders from each (train, val, test) tuple\n",
    "    test_loaders = [loaders[2] for loaders in site_loaders]\n",
    "\n",
    "    # Collect predictions and ground truth from each task\n",
    "    all_preds = [[] for _ in range(len(test_loaders))]\n",
    "    all_truth = [[] for _ in range(len(test_loaders))]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Zip all test loaders to get one batch per task at each iteration\n",
    "        for batches in zip(*test_loaders):\n",
    "            # For each task, extract inputs and targets\n",
    "            Xs = [batch[0].to(device) for batch in batches]\n",
    "            ys = [batch[1].to(device) for batch in batches]\n",
    "\n",
    "            # Forward pass for all tasks\n",
    "            preds_list = model(Xs)\n",
    "\n",
    "            # Accumulate predictions and ground truths\n",
    "            for i in range(len(test_loaders)):\n",
    "                all_preds[i].append(preds_list[i].cpu().numpy())\n",
    "                all_truth[i].append(ys[i].cpu().numpy())\n",
    "\n",
    "    # Now plot for each task\n",
    "    for i in range(len(test_loaders)):\n",
    "        # Concatenate along batch dimension and flatten\n",
    "        preds_i = np.concatenate(all_preds[i], axis=0).flatten()\n",
    "        truth_i = np.concatenate(all_truth[i], axis=0).flatten()\n",
    "\n",
    "        # Truncate to the same length, just in case\n",
    "        min_len = min(len(preds_i), len(truth_i))\n",
    "        preds_i = preds_i[:min_len]\n",
    "        truth_i = truth_i[:min_len]\n",
    "\n",
    "        # Build x-axes\n",
    "        # Ground truth covers indices [0, 1, 2, ..., min_len-1]\n",
    "        # Predictions are typically \"window_size\" steps ahead\n",
    "        time_axis = np.arange(min_len)\n",
    "        pred_axis = time_axis + window_size  # shift by window_size\n",
    "\n",
    "        # Downsample for plotting clarity (optional)\n",
    "        time_axis_plot = time_axis[::stride]\n",
    "        truth_plot = truth_i[::stride]\n",
    "        pred_axis_plot = pred_axis[::stride]\n",
    "        preds_plot = preds_i[::stride]\n",
    "\n",
    "        # Plot\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(time_axis_plot, truth_plot, label=\"Ground Truth\", linewidth=1)\n",
    "        plt.plot(pred_axis_plot, preds_plot, label=\"Predictions\", linewidth=1, linestyle='--')\n",
    "        plt.title(f\"Task {i+1} Predictions vs Ground Truth\")\n",
    "        plt.xlabel(\"Sample Index\")\n",
    "        plt.ylabel(\"Value\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    base_path = '../processed_ds/air_quality_cluster'\n",
    "    site_paths = [\n",
    "        os.path.join(root, file)\n",
    "        for root, dirs, files in os.walk(base_path)\n",
    "        if root != base_path  # Exclude files in the base directory\n",
    "        for file in files\n",
    "        if file.endswith(\".csv\")\n",
    "    ]\n",
    "\n",
    "    total_sites = len(site_paths)\n",
    "    features = ['PM2.5', 'OT', 'PM10', 'NO2']\n",
    "    target = 'PM2.5'\n",
    "    min_date = \"2014-09-01\"\n",
    "    max_date =  \"2014-11-12 19:00\"\n",
    "    num_tasks = total_sites\n",
    "    batch_size, window_size, input_dim, hidden_dim, output_dim = 32, 32, 4, 64, 16\n",
    "\n",
    "    model = MultiTaskFATHOM(num_tasks, input_dim, hidden_dim, output_dim, window_size).to(\"cpu\")\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.9)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    site_loaders = [load_and_preprocess_site_data(site_path, \n",
    "                                                  features, \n",
    "                                                  target, \n",
    "                                                  window_size, \n",
    "                                                  horizon=output_dim, \n",
    "                                                  batch_size=batch_size, \n",
    "                                                  min_date=min_date,\n",
    "                                                  max_date=max_date) for site_path in site_paths]\n",
    "\n",
    "    train_fathom_model(site_loaders, model, optimizer, criterion, scheduler, num_epochs=5, device=\"cpu\")\n",
    "    mae_scores = evaluate_fathom_model(model, site_loaders, device=\"cpu\")\n",
    "    print(f\"MAE per task: {mae_scores}\")\n",
    "    # Plot the predictions vs ground truth for each task\n",
    "    plot_predictions_vs_ground_truth(model, site_loaders, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Multiple-time-series",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
